{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "import shap as shap\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# Set up the connection details\n",
    "hostname = 'localhost'\n",
    "username = 'root'\n",
    "password = 'password'\n",
    "database = 'economics_of_sports'\n",
    "\n",
    "# Create a connection\n",
    "connection = pymysql.connect(host=hostname,\n",
    "                             user=username,\n",
    "                             password=password,\n",
    "                             db=database)\n",
    "\n",
    "# Create a cursor\n",
    "cursor = connection.cursor()\n",
    "\n",
    "query = '''\n",
    "select p.ws_id, tmvr.market_value, p.name, p.nationality, p.continent,  p.heigt , tmvr.player_age, p.nationality = l.country , tmvr.season,\n",
    "    IF(LOCATE(' (', p.posistion) = 0, p.posistion, LEFT(p.posistion, LOCATE(' (', p.posistion) - 1)) AS posistion,\n",
    "       ws_nl.tournament, ws_nl.elo_score, ws_nl.rating, ws_nl.minutes_played, ws_nl.motm_awards\n",
    "        #,ws_ec.tournament, ws_ec.elo_score, ws_ec.rating, ws_ec.minutes_played, ws_ec.motm_awards\n",
    "        #,ws_nl.tournament, ws_ntc.elo_score, ws_ntc.rating, ws_ntc.minutes_played, ws_ntc.motm_awards\n",
    "        #,ws_nc.tournament, ws_nc.elo_score, ws_nc.rating, ws_nc.minutes_played, ws_nc.motm_awards\n",
    "from player p\n",
    "left join tm_market_value_raw tmvr on p.tm_id = tmvr.tm_player_id\n",
    "    left join team t on tmvr.team_name = t.name\n",
    "left join league l on t.league_id = l.id\n",
    "left join (\n",
    "    select * from ws_season\n",
    "             where type = 'National Leagues'\n",
    ") as ws_nl on ws_nl.ws_player_id = p.ws_id and ws_nl.season = tmvr.season\n",
    "left join (\n",
    "    select * from ws_season\n",
    "             where type = 'European Cups'\n",
    ") as ws_ec on ws_ec.ws_player_id = p.ws_id and ws_ec.season = tmvr.season\n",
    "left join (\n",
    "    select * from ws_season\n",
    "             where type = 'National Team Cups'\n",
    ") as ws_ntc on ws_ntc.ws_player_id = p.ws_id and ws_ntc.season = tmvr.season\n",
    "left join (\n",
    "    select * from ws_season\n",
    "             where type = 'National Cups'\n",
    ") as ws_nc on ws_nc.ws_player_id = p.ws_id and ws_nc.season = tmvr.season\n",
    "where ws_nl.season is not null\n",
    "group by posistion, ws_nl.tournament, tmvr.market_value, p.name, p.nationality, p.continent, p.heigt, tmvr.player_age, p.nationality = l.country, tmvr.season, p.ws_id, ws_nl.elo_score, ws_nl.rating, ws_nl.minutes_played, ws_nl.motm_awards, ws_ec.tournament, ws_ec.elo_score, ws_ec.rating, ws_ec.minutes_played, ws_ec.motm_awards, ws_nl.tournament, ws_ntc.elo_score, ws_ntc.rating, ws_ntc.minutes_played, ws_ntc.motm_awards, ws_nc.tournament, ws_nc.elo_score, ws_nc.rating, ws_nc.minutes_played, ws_nc.motm_awards\n",
    "order by name, tmvr.season'''\n",
    "\n",
    "# Execute a query to select all from the \"employees\" table\n",
    "cursor.execute(query=query)\n",
    "\n",
    "# Fetch all the rows\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Get column names\n",
    "column_names = [i[0] for i in cursor.description]\n",
    "\n",
    "# Convert the data to a pandas DataFrame\n",
    "df = pd.DataFrame(rows, columns=column_names)\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "connection.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Clean up data and split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "targets=df.market_value\n",
    "df[[\"rating\"]] = df[[\"rating\"]].apply(pd.to_numeric)\n",
    "\n",
    "inputs=df.drop(['ws_id', 'name', 'tournament', 'market_value'],axis='columns')\n",
    "\n",
    "inputs = pd.get_dummies(inputs)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, targets, test_size=0.25, random_state=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create train model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.701514943323311\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg = RandomForestRegressor()\n",
    "reg.fit(X_train, y_train)\n",
    "train_sq = reg.score(X_train, y_train)\n",
    "r_sq = reg.score(X_test, y_test)\n",
    "print(r_sq)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
      "Training score:  0.6798960232873827\n",
      "Testing score:  0.5510948884373894\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200, 500],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth' : [2,4,6,8,10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "}\n",
    "\n",
    "reg = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=reg, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print the best parameters\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "\n",
    "# calculate the training and testing score using the best model\n",
    "best_grid = grid_search.best_estimator_\n",
    "\n",
    "train_sq = best_grid.score(X_train, y_train)\n",
    "r_sq = best_grid.score(X_test, y_test)\n",
    "\n",
    "print(\"Training score: \", train_sq)\n",
    "print(\"Testing score: \", r_sq)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analyse model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This RandomForestRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotFittedError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [62]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Get feature importances\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m importances \u001B[38;5;241m=\u001B[39m \u001B[43mreg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature_importances_\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Create a pandas DataFrame with feature importances\u001B[39;00m\n\u001B[1;32m      5\u001B[0m importance_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFeature\u001B[39m\u001B[38;5;124m\"\u001B[39m: X_train\u001B[38;5;241m.\u001B[39mcolumns, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mImportance\u001B[39m\u001B[38;5;124m\"\u001B[39m: importances})\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/kandidat_kode/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:628\u001B[0m, in \u001B[0;36mBaseForest.feature_importances_\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    607\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    608\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeature_importances_\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    609\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    610\u001B[0m \u001B[38;5;124;03m    The impurity-based feature importances.\u001B[39;00m\n\u001B[1;32m    611\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    626\u001B[0m \u001B[38;5;124;03m        array of zeros.\u001B[39;00m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m     \u001B[43mcheck_is_fitted\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    630\u001B[0m     all_importances \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs, prefer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthreads\u001B[39m\u001B[38;5;124m\"\u001B[39m)(\n\u001B[1;32m    631\u001B[0m         delayed(\u001B[38;5;28mgetattr\u001B[39m)(tree, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfeature_importances_\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m tree \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m tree\u001B[38;5;241m.\u001B[39mtree_\u001B[38;5;241m.\u001B[39mnode_count \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    634\u001B[0m     )\n\u001B[1;32m    636\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m all_importances:\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/kandidat_kode/lib/python3.9/site-packages/sklearn/utils/validation.py:1390\u001B[0m, in \u001B[0;36mcheck_is_fitted\u001B[0;34m(estimator, attributes, msg, all_or_any)\u001B[0m\n\u001B[1;32m   1385\u001B[0m     fitted \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   1386\u001B[0m         v \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mvars\u001B[39m(estimator) \u001B[38;5;28;01mif\u001B[39;00m v\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m v\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1387\u001B[0m     ]\n\u001B[1;32m   1389\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fitted:\n\u001B[0;32m-> 1390\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NotFittedError(msg \u001B[38;5;241m%\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(estimator)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m})\n",
      "\u001B[0;31mNotFittedError\u001B[0m: This RandomForestRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "importances = reg.feature_importances_\n",
    "\n",
    "# Create a pandas DataFrame with feature importances\n",
    "importance_df = pd.DataFrame({\"Feature\": X_train.columns, \"Importance\": importances})\n",
    "\n",
    "# Sort the DataFrame by importances\n",
    "importance_df = importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Summarize feature importances\n",
    "for i, row in importance_df.iterrows():\n",
    "    print(f'Feature {row[\"Feature\"]}: {row[\"Importance\"]}')\n",
    "\n",
    "# Plot feature importances\n",
    "plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature importances\")\n",
    "plt.gca().invert_yaxis()  # Invert the y-axis to show the most important at the top\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
